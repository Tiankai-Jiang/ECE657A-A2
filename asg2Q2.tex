\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{changepage} 

% for line spacing
\usepackage{setspace}

% for absolute value
\usepackage{commath}

% for numbering
\usepackage{enumerate}

% for image placing
\usepackage{float}

% paper size and margins
\usepackage[letterpaper, left=20mm, right=20mm, top=25mm, bottom = 25mm, headsep=.15in]{geometry}

% for code snippet
\usepackage{listings}

% for curly brace
\usepackage{amsmath}

% for input images
\usepackage{graphicx}
\graphicspath{ {./} }
\usepackage{subfig}

% for printing pseudocode
\usepackage[boxed]{algorithm}
\usepackage[noend]{algpseudocode}

% for tables
\usepackage{tabularx}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

% for circled numbers
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=1pt] (char) {#1};}}

% double line space
\renewcommand{\baselinestretch}{2.0}

% header, footer and page number
\pagestyle{fancy}
\fancyhf{}
\rhead{Tiankai Jiang \quad 20834939}
\lhead{ECE657A \quad Assignment 2}
\fancyfoot[C]{\thepage}

\setlength{\headheight}{15pt}
\lstset{language=Python}
\begin{document}
\noindent
\textbf{\large Question 2:}\\
\textbf{Scree Plot}
\begin{center}
    \includegraphics[width=14cm]{Q2_ScreePlot.png}
\end{center}
\noindent
According to the scree plot, the explained variance drops sharply from 0 to 50. A cut-off around 10 is good since after that, the amount of variance explained by the selected component is relatively small.\\\\
\textbf{Projected Data With Top 20 Eigenvalues in PCA}
\begin{center}
    \includegraphics[width=18cm]{Q2_PCATop19.png}
\end{center}
\noindent
We can see some clear separations between classes in the first 6 plots(PC1 vs PC2 to PC5 vs PC6). But after that almost all points are concentrated in the middle and we cannot tell them apart. Hence, keeping the first 6 components is a good choice. It is clearer to find the cut-off using pair plot since the slope is too large in the scree plot, making it difficult to find a specific cut-off value.\\\\
\textbf{PC1,2 Compared to PC5,6}
\begin{figure}[H]
\captionsetup[subfigure]{labelformat=empty}
\centering
\subfloat[]{\includegraphics[width=0.48\textwidth]{Q2_PCA01.png}}
\subfloat[] {\includegraphics[width=0.48\textwidth]{Q2_PCA45.png}}
\end{figure}
\vspace*{-1.5cm}
\noindent
The first plot(PC1 vs PC2) mainly separates number 0 from other classes. It is difficult to distinguish the other 4 classes from the first plot as they all cluster on the left. Actually, PC2 does not help much to classify number 0, since the border between number 0 and others are almost vertical on the graph(PC1 = 10).\\
The second plot(PC5 vs PC6) makes some contribution to separating number 2 and number 3 from other classes. But it is not sufficient to use only PC5 and PC6 since there is large overlap in the middle between all numbers.\\
Although the first plot seems more orderless than the second plot, we can use it along to separate one class. But we cannot separate classes using only the second plot.\\\\
\textbf{SVD, PCA and Dual PCA Implementation}
\begin{spacing}{1.0}
\begin{lstlisting}
# Singular value decomposition
# input: an n*t array, n is the dimensionality and t is 
# the number of training examples
def svd_np(x):
    eigValueU, eigVectorU = np.linalg.eigh(np.dot(x, x.T))
    eigValueV, eigVectorV = np.linalg.eigh(np.dot(x.T, x))
    idxU = eigValueU.argsort()[::-1]
    idxV = eigValueV.argsort()[::-1]
    return eigVectorU[:,idxU], 
    np.sqrt(eigValueV[idxV] if x.shape[0] > x.shape[1] else eigValueU[idxU]), 
    eigVectorV[:,idxV].T
\end{lstlisting}
\begin{lstlisting}
# First get U, sigma and Vt from SVD
# s is a t*n array
U, sigma, Vt = svd_np(s.T)

# PCA
s.dot(U)

# Dual PCA
np.diag(sigma).dot(Vt[:s.shape[1],:]).T
\end{lstlisting}
\end{spacing}
\noindent
Assume that we have already known the value of $U$, $\Sigma$ and $V^T$, run PCA and dual PCA 1000 times each and get the average runtime.\\
PCA: 22359.35 microseconds\\
Dual PCA: 22124.32 microseconds\\
Under this specific dataset, their performance is basically the same.\\\\
\textbf{Prove that PCA is the best linear method for reconstruction}\\
Optimization problem: maximize $tr(U^TSU)$ s.t. $U^TU=I$, where $S=\overline{X}\;\overline{X}^T$ and $\overline{X}$ is the normalized input data.\\
In other words, maximize the variance of the projection under constraint $U^TU=I$.
\begin{equation}
\begin{split}
\mathcal{L}(U,\lambda) & = tr(U^TSU) - tr(\lambda(U^TU-I)) \\
\frac{\partial \mathcal{L}}{\partial U} & = 2SU-2\lambda U \\\\
let\;\frac{\partial \mathcal{L}}{\partial U} &=0 \\
SU & = \lambda U 
\end{split}
\end{equation}
\noindent
The result indicates that to maximize the equation, U should take the value of the eigenvector of S with the largest eigenvalue.\\
\textbf{Projected Data With Top 4 Eigenvalues in LDA}
\vspace*{-1.0cm}
\begin{center}
    \includegraphics[width=18cm]{Q2_LDATop3.png}
\end{center}
\noindent
LDA1 separates number 0 and number 1. LDA2 separates number 4. LDA2 separates number 3 and LDA4 separates number 2.\\
Each LDA direction is mainly responsible for separating one or two classes, but that is not the case with each principle component.\\\\
\textbf{Compare LDA Optimization With PCA Optimization}
\end{document}